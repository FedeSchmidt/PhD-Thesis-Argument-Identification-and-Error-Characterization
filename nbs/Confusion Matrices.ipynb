{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d80afa-5550-4b79-b73f-8b803118613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = Path().resolve().parent\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca7384c-62fe-49f9-a053-87f4fcaa132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    instances = []\n",
    "    try:\n",
    "        with open(file_path, encoding='utf-8') as nf:\n",
    "            lines = nf.readlines()\n",
    "            tokens, labels, predictions = [], [], []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    tk, lb, pr = line.split()\n",
    "                    tokens.append(tk)\n",
    "                    labels.append(lb)\n",
    "                    predictions.append(pr)\n",
    "                else:\n",
    "                    instances.append((tokens, labels, predictions))\n",
    "                    tokens, labels, predictions = [], [], []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        print(f\"Invalid file format: {file_path}\")\n",
    "\n",
    "    return instances\n",
    "\n",
    "def get_best_run_number(row):\n",
    "    columns = [f'run{i}' for i in range(10)]\n",
    "    values = row[columns].values\n",
    "    return np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051f73f9-cfd5-46a0-b7c8-64904f92bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the tokenizer may divide each token into two or more subtokens, the predictions may contain subtokens.\n",
    "# Subtokens' labels are equal to the label of parent token.\n",
    "# We must join subtokens with their parent tokens for obtaining the same amount of tokens as original dataset.\n",
    "def format_instances(tokens, labels, predictions):\n",
    "    output_tokens, output_labels, output_predictions = [], [], []\n",
    "    for instance_tokens, instance_labels, instance_predictions in zip(tokens, labels, predictions):\n",
    "\n",
    "        clean_tokens = []\n",
    "        clean_labels = []\n",
    "        clean_preds = []\n",
    "        current_parent_token = \"\"\n",
    "        parent_token_label = '0'\n",
    "        parent_token_pred = '0'\n",
    "\n",
    "        for t, l, p in zip(instance_tokens, instance_labels, instance_predictions):\n",
    "            if t.startswith(\"##\"):\n",
    "                current_parent_token += t[2:]\n",
    "            else:\n",
    "                if current_parent_token != \"\":\n",
    "                    clean_tokens.append(current_parent_token)\n",
    "                    clean_labels.append(parent_token_label)\n",
    "                    clean_preds.append(parent_token_pred)\n",
    "\n",
    "                current_parent_token = t\n",
    "                parent_token_label = l\n",
    "                parent_token_pred = p\n",
    "\n",
    "        output_tokens.append(clean_tokens)\n",
    "        output_labels.append(clean_labels)\n",
    "        output_predictions.append(clean_preds)\n",
    "\n",
    "    return output_tokens, output_labels, output_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72648ebe-a2c4-46df-951d-6d863c32986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_results = pd.read_csv(str(current_path / 'data.csv'))\n",
    "f1_results['best_run'] = f1_results.apply(lambda row: get_best_run_number(row), axis = 1)\n",
    "# print(f1_results.shape)\n",
    "# f1_results.head()\n",
    "\n",
    "def get_labels_and_predictions(df, elem, test, model):\n",
    "\n",
    "    best_run = df.loc[(df['train'] == elem) & (df['test'] == test) & (df['model'] == model)].best_run.values[0]\n",
    "    \n",
    "    folder_dir = f\"{elem}_{model}\"\n",
    "    model_name = \"-\".join(model.split(\"_\"))\n",
    "    file_name = f\"results-{elem}-{test}-{model_name}-{best_run}.txt\"\n",
    "    file_path = (current_path / 'results' / 'BIO' / folder_dir / file_name)\n",
    "    \n",
    "    instances = parse_file(file_path)\n",
    "\n",
    "    tokens = [x[0] for x in instances]\n",
    "    labels = [x[1] for x in instances]\n",
    "    predictions = [x[2] for x in instances]\n",
    "    \n",
    "    new_tokens, new_labels, new_predictions = format_instances(tokens, labels, predictions)\n",
    "    \n",
    "    # Task 1: Replace '0' with 'O', '1' with 'B', and '2' with 'I'\n",
    "    labels = [['O' if x == '0' else 'B' if x == '1' else 'I' for x in inner_list] for inner_list in new_labels]\n",
    "    predictions = [['O' if x == '0' else 'B' if x == '1' else 'I' for x in inner_list] for inner_list in new_predictions]\n",
    "    \n",
    "    # Task 2: Flatten the list of lists\n",
    "    flattened_labels = [label for inner_list in labels for label in inner_list]\n",
    "    flattened_predictions = [label for inner_list in predictions for label in inner_list]\n",
    "    \n",
    "    return flattened_labels, flattened_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1fe67-cde7-4eaf-a553-2bc3ebf4d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels1, predictions1 = get_labels_and_predictions(f1_results, 'pe', 'pe', 'bert_crf')\n",
    "labels2, predictions2 = get_labels_and_predictions(f1_results, 'we', 'we', 'bert_crf')\n",
    "labels3, predictions3 = get_labels_and_predictions(f1_results, 'abam', 'abam', 'bert_crf')\n",
    "\n",
    "# Function to plot a confusion matrix\n",
    "def plot_confusion_matrix(ax, labels_true, labels_pred, title):\n",
    "    cm = confusion_matrix(labels_true, labels_pred)\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    cm_row_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    cm_overall_norm = cm.astype('float') / cm.sum() * 100\n",
    "    sns.heatmap(cm_row_norm, annot=True, fmt=\".1f\", cmap=\"Blues\", ax=ax, cbar = False, annot_kws={\"size\": 14})\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicho')\n",
    "    ax.set_xticklabels(['B', 'I', 'O'])\n",
    "    ax.set_yticklabels(['B', 'I', 'O'])\n",
    "    ax.set_ylabel('Correcto')\n",
    "\n",
    "# # Create subplots for three confusion matrices in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# # Plot each confusion matrix\n",
    "plot_confusion_matrix(axes[0], labels1, predictions1, 'PE')\n",
    "plot_confusion_matrix(axes[1], labels2, predictions2, 'WE')\n",
    "plot_confusion_matrix(axes[2], labels3, predictions3, 'ABAM')\n",
    "\n",
    "# # Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(str((current_path) / 'figures' / 'ch3-confugsionMatrices.pdf'), format='pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
